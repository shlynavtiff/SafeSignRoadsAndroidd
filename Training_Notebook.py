# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mfX_lQMHar6x_x9ZkAani5nJGhKe5Jue
"""

!unzip sounds.zip

# Cell 1: Load, Preprocess (Chunk/Pad/Trim), Normalize

import librosa
import numpy as np
import os
import time
import warnings

# Suppress potential UserWarnings from librosa about audioread/ffmpeg backend
warnings.filterwarnings('ignore', category=UserWarning, module='librosa')

# --- Configuration ---
data_dir = '/content/sounds/' # <<< CONFIRM this points to your dataset folder
target_sr = 22050
target_duration = 3.0 # Target length in seconds
target_length_samples = int(target_duration * target_sr) # Target length in samples
# *** IMPORTANT: Update this list to match your 3 classes and folder names ***
categories = ['car_horn', 'emergency_vehicle', 'traffic']
# ************************************************************************

# --- Variables ---
audio_data_final = [] # List to store final 3-second, normalized chunks
labels = []
file_counts = {category: 0 for category in categories}
errors = []

print(f"Starting audio loading, chunking, and normalization from: {data_dir}")
start_time = time.time()

# --- Helper function for peak normalization ---
def normalize_peak(y):
    peak = np.max(np.abs(y))
    # Use a small epsilon to avoid division by near-zero for silent chunks
    if peak > 1e-5:
        return y / peak
    else:
        # print("Warning: audio seems silent or near-silent, not normalizing.")
        return y

# --- Helper function for finding highest energy chunk ---
def get_max_energy_chunk(y, sr, chunk_len_samples, target_len_samples):
    if len(y) <= target_len_samples:
         # Should not happen if called correctly, but pad just in case
         pad_width = target_len_samples - len(y)
         return np.pad(y.astype(np.float32), (0, pad_width), mode='constant', constant_values=0.0)

    # Calculate energy (RMS) in short frames
    frame_length = 2048
    hop_length = 512
    rms_energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]

    # Find the frame with maximum energy
    max_energy_frame_index = np.argmax(rms_energy)

    # Calculate the center sample corresponding to the max energy frame
    center_sample = librosa.frames_to_samples(max_energy_frame_index, hop_length=hop_length)

    # Calculate start/end samples for the target chunk length, centered
    start_sample = max(0, center_sample - (target_len_samples // 2))
    end_sample = start_sample + target_len_samples

    # Adjust if window goes beyond audio boundaries
    if end_sample > len(y):
        end_sample = len(y)
        start_sample = max(0, end_sample - target_len_samples)

    # Extract the chunk
    chunk = y[start_sample:end_sample]

    # Final check/fix length (padding if extraction resulted slightly short)
    current_len = len(chunk)
    if current_len < target_len_samples:
         pad_width = target_len_samples - current_len
         return np.pad(chunk.astype(np.float32), (0, pad_width), mode='constant', constant_values=0.0)
    elif current_len > target_len_samples: # Should not happen, but safety truncate
         return chunk[:target_len_samples].astype(np.float32)
    else:
         return chunk.astype(np.float32)


# --- Main Loading Loop ---
if not os.path.exists(data_dir):
    print(f"ERROR: Directory not found: {data_dir}")
else:
    for category in categories:
        category_dir = os.path.join(data_dir, category)
        print(f"\nProcessing category: {category}...")

        if not os.path.exists(category_dir):
            print(f"  WARNING: Category directory not found: {category_dir}")
            continue

        files_in_category = [f for f in os.listdir(category_dir) if f.lower().endswith(('.wav', '.mp3'))]
        print(f"  Found {len(files_in_category)} audio files.")

        for filename in files_in_category:
            file_path = os.path.join(category_dir, filename)
            try:
                # 1. Load FULL audio file first
                y, sr = librosa.load(file_path, sr=target_sr, mono=True) # Ensure mono

                # 2. Select/Fix Chunk based on length and category
                if len(y) > target_length_samples and category == 'car_horn':
                    # If it's a car horn longer than 3s, find the loudest 3s chunk
                    # print(f"    Extracting max energy 3s chunk from {filename} (len: {len(y)/sr:.2f}s)") # Optional verbose log
                    final_chunk = get_max_energy_chunk(y, sr, target_length_samples, target_length_samples)
                else:
                    # For other categories OR car horns <= 3s, just pad/truncate to 3s
                     current_len = len(y)
                     if current_len > target_length_samples:
                         final_chunk = y[:target_length_samples]
                     elif current_len < target_length_samples:
                         pad_width = target_length_samples - current_len
                         final_chunk = np.pad(y.astype(np.float32), (0, pad_width), mode='constant', constant_values=0.0)
                     else:
                         final_chunk = y
                     # Ensure float32
                     final_chunk = final_chunk.astype(np.float32)

                # 3. Normalize the final 3-second chunk
                y_normalized = normalize_peak(final_chunk)

                # 4. Append final processed data
                audio_data_final.append(y_normalized)
                labels.append(category)
                file_counts[category] += 1

            except Exception as e:
                error_msg = f"  Error processing {file_path}: {e}"
                print(error_msg)
                errors.append(error_msg)

    end_time = time.time()
    print("\n------------------------------------")
    print("Audio Loading / Chunking / Normalization Summary:")
    print(f"Total time taken: {end_time - start_time:.2f} seconds")
    print(f"Successfully processed {len(audio_data_final)} files.")
    print("Files processed per category:")
    for category_name in categories: # Iterate through defined categories order
         count = file_counts.get(category_name, 0) # Use get in case a category had 0 files
         print(f"  - {category_name}: {count}")


    if errors:
        print(f"\nEncountered {len(errors)} errors during processing:")
        # for err in errors: print(err) # Optionally print all errors
    else:
        print("\nNo errors encountered during processing.")

    # Assign to expected variable name for subsequent cells
    audio_data = audio_data_final
    # Make sure subsequent cells know the data structure (list of numpy arrays)
    # And make sure the labels list also has the correct length
    print(f"\nProcessed audio stored in 'audio_data' (Length: {len(audio_data)})")
    print(f"Labels stored in 'labels' (Length: {len(labels)})")
    print("Ready for Feature Extraction.")
    print("------------------------------------")

# Cell 2: Feature Extraction (MFCC)
# Based on cells [19-24] from Audio Training Notebook - Copy.txt

import librosa
import numpy as np
import time

# Make sure audio_data, target_sr, and target_length_samples exist from Cell 1
if 'audio_data' not in locals() or 'target_sr' not in locals() or 'target_length_samples' not in locals():
    print("ERROR: Ensure Cell 1 (Data Loading) has been run successfully first!")
else:
    print(f"Extracting MFCC features (using librosa defaults)...")
    start_time = time.time()

    mfcc_features = []

    # Default librosa settings often use n_mfcc=20
    # For 3s audio at 22050 Hz with hop_length=512, expect ~130 frames
    # These values are based on librosa defaults and the target_length_samples from Cell 1
    n_mfcc_expected = 20
    n_frames_expected = int(np.ceil(target_length_samples / 512)) # Default hop_length=512

    for i, y in enumerate(audio_data):
        # Calculate MFCCs. Uses default n_fft=2048, hop_length=512, n_mfcc=20
        # Ensure input 'y' is float32, which it should be from normalization step
        mfcc = librosa.feature.mfcc(y=y.astype(np.float32), sr=target_sr, n_mfcc=n_mfcc_expected, n_fft=2048, hop_length=512)
        mfcc_features.append(mfcc)


    end_time = time.time()
    print(f"\nFinished extracting features in {end_time - start_time:.2f} seconds.")
    print(f"Created list 'mfcc_features' with {len(mfcc_features)} feature sets.")

    # --- Verification ---
    # Check the shape of the first few MFCC arrays
    print("\nVerifying shapes of extracted MFCC features:")
    all_shapes_correct = True
    # Expected shape: (n_mfcc, n_frames)
    correct_shape = (n_mfcc_expected, n_frames_expected) # Approximately (20, 130)

    if not mfcc_features:
         print("  No features were extracted.")
    else:
        for i in range(min(5, len(mfcc_features))):
            current_shape = np.shape(mfcc_features[i])
            print(f"  - MFCC Set {i}: shape={current_shape}")
            # Check if shape matches expected (allow slight frame variation if any)
            # Frame count can sometimes differ by 1 due to padding specifics
            if current_shape[0] != correct_shape[0] or abs(current_shape[1] - correct_shape[1]) > 1 :
                 all_shapes_correct = False
                 print(f"    WARNING: Unexpected shape! Expected around {correct_shape}")

        if all_shapes_correct:
            print(f"\nAll checked samples have the expected MFCC shape (approx. {correct_shape}).")
        else:
             print("\nWARNING: Some samples did not have the expected MFCC shape. Check consistency.")

    print("\nReady for Label Encoding.")
    print("------------------------------------")

# Cell 2b: Preview Processed Audio Samples

import IPython.display as ipd
import numpy as np
import random

# Ensure audio_data, labels, and target_sr exist from Cell 1
if 'audio_data' not in locals() or 'labels' not in locals() or 'target_sr' not in locals():
     print("ERROR: Ensure Cell 1 (Data Loading/Processing) has run successfully first!")
else:
    print("Previewing up to 5 random processed 'car_horn' samples...")
    print("(These should be normalized and 3 seconds long)")

    # Find indices of car_horn samples
    car_horn_indices = [i for i, label in enumerate(labels) if label == 'car_horn']

    if not car_horn_indices:
        print("No 'car_horn' samples found in the processed data.")
    else:
        # Select up to 5 random samples to preview
        num_previews = min(5, len(car_horn_indices))
        preview_indices = random.sample(car_horn_indices, num_previews)

        for index in preview_indices:
            print(f"\nPlaying Processed Car Horn Sample (Original Index {index}):")
            # Get the processed audio data (should be 3s numpy array)
            audio_sample = audio_data[index]
            # Check length in samples (should be target_length_samples = 66150)
            print(f"  - Sample Length: {len(audio_sample)} samples")
            # Check amplitude range (should be roughly within [-1, 1] due to normalization)
            print(f"  - Amplitude Range: [{np.min(audio_sample):.2f}, {np.max(audio_sample):.2f}]")

            # Create an interactive audio player in Colab output
            display(ipd.Audio(audio_sample, rate=target_sr, normalize=False)) # Don't re-normalize display
            print("-" * 30)

    print("\nPreview finished.")
    print("------------------------------------")

# Cell 3: Label Encoding
# Based on cells [25-26] from Audio Training Notebook - Copy.txt

import numpy as np
from sklearn.preprocessing import LabelEncoder
import collections # To check distribution

# Ensure mfcc_features and labels exist from previous cells
if 'mfcc_features' not in locals() or 'labels' not in locals():
     print("ERROR: Ensure previous cells (Data Loading, MFCC Extraction) have run successfully!")
else:
    # Convert features list to a NumPy array first
    # This was previously done later, but better to do it here
    # Expected shape: (num_samples, num_coefficients, num_frames) -> (1200, 20, 130)
    X = np.array(mfcc_features)
    if X.shape[0] != len(labels):
         print(f"WARNING: Number of feature sets ({X.shape[0]}) does not match number of labels ({len(labels)})!")

    # Convert labels list to a NumPy array
    y_text = np.array(labels)

    # Encode text labels to integers (0, 1, 2 based on alphabetical order)
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y_text) # Fit and transform in one step

    print("Data shapes after encoding:")
    print(f"Features (X) shape: {X.shape}")
    print(f"Labels (y) shape: {y.shape}") # Should be (1200,)

    print("\nLabel Encoding Mapping:")
    # Print the mapping found by LabelEncoder
    for index, label_name in enumerate(label_encoder.classes_):
        print(f"  {index}: {label_name}")

    print("\nDistribution of original text labels:")
    print(collections.Counter(y_text))
    print("\nDistribution of encoded integer labels:")
    print(collections.Counter(y)) # Check if counts match (e.g., 600, 400, 200)

    print("\nReady for Data Splitting.")
    print("------------------------------------")

    # Optional: Save the fitted encoder NOW if you want to be sure
    # import joblib
    # encoder_filename = 'label_encoder_3class.joblib'
    # try:
    #     joblib.dump(label_encoder, encoder_filename)
    #     print(f"LabelEncoder saved to {encoder_filename}")
    # except Exception as e:
    #     print(f"Error saving LabelEncoder: {e}")

# Cell 4: Data Splitting
# Based on cells [27-31] from Audio Training Notebook - Copy.txt

from sklearn.model_selection import train_test_split
import collections # To check distribution again

# Ensure X and y exist from the previous cell
if 'X' not in locals() or 'y' not in locals():
     print("ERROR: Ensure Cell 3 (Label Encoding) has been run successfully first!")
else:
    # Define split proportions
    test_size = 0.15  # Reserve 15% for the final unseen test set
    # Calculate validation size relative to the *remaining* data to get ~15% of original
    # e.g. 0.15 / (1.0 - 0.15) = 0.15 / 0.85 = ~0.1765
    val_size_relative = 0.1765

    # Set a random state for reproducible splits
    random_seed = 42

    print("Splitting data into Train, Validation, and Test sets...")
    print(f"Targeting roughly: 70% Train, 15% Validation, 15% Test")

    # First split: Separate Test Set (15%)
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, # Use the full dataset X, y from previous cell
        test_size=test_size,
        random_state=random_seed,
        stratify=y # Ensures proportions are maintained in the split
    )
    print(f"\nIntermediate split: Temp set = {X_temp.shape[0]} samples, Test set = {X_test.shape[0]} samples.")

    # Second split: Separate Training (70% of original) and Validation (15% of original) from the temporary set
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, # Split the remaining 85%
        test_size=val_size_relative, # This fraction of the temp set gives ~15% of original total
        random_state=random_seed,
        stratify=y_temp # Ensure stratification on this split too
    )

    print(f"Final split: Train set = {X_train.shape[0]} samples, Validation set = {X_val.shape[0]} samples.")

    print("\n--- Data Splitting Summary ---")
    print("Shapes of the resulting feature sets:")
    print(f"  Training Features (X_train):   {X_train.shape}")
    print(f"  Validation Features (X_val):   {X_val.shape}")
    print(f"  Test Features (X_test):      {X_test.shape}")
    print("\nShapes of the resulting label sets:")
    print(f"  Training Labels (y_train):     {y_train.shape}")
    print(f"  Validation Labels (y_val):     {y_val.shape}")
    print(f"  Test Labels (y_test):        {y_test.shape}")

    # Verify total adds up
    total_split = len(y_train) + len(y_val) + len(y_test)
    print(f"\nTotal samples in splits: {total_split} (Original: {len(y)})")
    if total_split != len(y):
         print("WARNING: Total samples in splits do not match original dataset size!")


    print("\nLabel distribution in each set:")
    # Use np.bincount for potentially simpler distribution view if labels are 0, 1, 2...
    # Or stick with collections.Counter
    try:
        train_counts = collections.Counter(y_train)
        val_counts = collections.Counter(y_val)
        test_counts = collections.Counter(y_test)
        print(f"  Train:      {sorted(train_counts.items())}") # Sort by index for consistency
        print(f"  Validation: {sorted(val_counts.items())}")
        print(f"  Test:       {sorted(test_counts.items())}")
    except Exception as e:
         print(f"Could not calculate label distributions: {e}")


    print("\nReady for Data Reshaping for CNN.")
    print("------------------------------------")

# Cell 5: Reshape Data for CNN Input
# Based on cells [32-34] from Audio Training Notebook - Copy.txt

import numpy as np

# Ensure X_train, X_val, X_test exist from the previous cell
if 'X_train' not in locals() or 'X_val' not in locals() or 'X_test' not in locals():
     print("ERROR: Ensure Cell 4 (Data Splitting) has been run successfully first!")
else:
    print("Original shapes:")
    print(f"  X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}")

    # Reshape by swapping the last two dimensions (n_mfcc, n_frames) -> (n_frames, n_mfcc)
    # This matches the (batch_size, steps, features) format expected by Conv1D
    X_train_cnn = X_train.transpose(0, 2, 1)
    X_val_cnn = X_val.transpose(0, 2, 1)
    X_test_cnn = X_test.transpose(0, 2, 1)

    # The labels (y_train, y_val, y_test) remain unchanged

    print("\nNew shapes for CNN input:")
    print(f"  X_train_cnn: {X_train_cnn.shape}") # Should be (num_samples, 130, 20)
    print(f"  X_val_cnn:   {X_val_cnn.shape}")   # Should be (num_samples, 130, 20)
    print(f"  X_test_cnn:  {X_test_cnn.shape}")  # Should be (num_samples, 130, 20)

    # Define the input shape required by the first layer of the Keras model
    # It doesn't include the batch size dimension
    input_shape_cnn = (X_train_cnn.shape[1], X_train_cnn.shape[2]) # Should be (130, 20)
    print(f"\nInput shape variable for Keras model ('input_shape_cnn'): {input_shape_cnn}")

    print("\nReady for Model Definition.")
    print("------------------------------------")

# Cell 6: Define the CNN Model Architecture
# Based on cells [35-38] from Audio Training Notebook - Copy.txt

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Ensure input_shape_cnn exists from the previous cell
if 'input_shape_cnn' not in locals():
     print("ERROR: Ensure Cell 5 (Data Reshaping) has been run successfully first!")
     # Define fallback if needed for testing, but previous cell should be run
     # input_shape_cnn = (130, 20) # Example fallback
else:
     # Define number of output classes explicitly
     num_classes = 3 # For car_horn, emergency_vehicle, traffic

     print(f"Building a 1D CNN model for {num_classes} classes...")
     print(f"Expected input shape (steps, features): {input_shape_cnn}")

     model = keras.Sequential(
         [
             # Input Layer specifying the shape (steps, features)
             keras.Input(shape=input_shape_cnn, name="input_layer"),

             # --- Convolutional Block 1 ---
             layers.Conv1D(filters=64, kernel_size=3, activation="relu", padding="same", name="conv1d_1"),
             layers.BatchNormalization(name="batchnorm_1"), # Helps stabilize training
             layers.MaxPooling1D(pool_size=2, name="maxpool_1"),
             # layers.Dropout(0.2), # Optional dropout

             # --- Convolutional Block 2 ---
             layers.Conv1D(filters=128, kernel_size=3, activation="relu", padding="same", name="conv1d_2"),
             layers.BatchNormalization(name="batchnorm_2"),
             layers.MaxPooling1D(pool_size=2, name="maxpool_2"),
             # layers.Dropout(0.2), # Optional dropout

             # --- Global Pooling ---
             # Condenses the sequence information
             layers.GlobalMaxPooling1D(name="global_maxpool"),
             # Alternative: layers.GlobalAveragePooling1D(),

             # --- Classification Head ---
             layers.Dropout(0.4, name="dropout_1"), # Higher dropout before final dense layers
             layers.Dense(64, activation="relu", name="dense_1"),
             layers.Dropout(0.4, name="dropout_2"),

             # *** IMPORTANT: Output layer MUST have 3 units for your 3 classes ***
             layers.Dense(num_classes, activation="softmax", name="output_layer"),
             # ****************************************************************
         ],
         name="audio_classifier_cnn" # Optional name for the model
     )

     # Print the model summary
     model.summary()

     print("\nModel definition complete.")
     print("------------------------------------")

# Cell 7: Compile the Model
# Based on cells [39-40] from Audio Training Notebook - Copy.txt

import tensorflow as tf
from tensorflow import keras

# Ensure the 'model' variable exists from the previous cell
if 'model' not in locals():
     print("ERROR: Ensure Cell 6 (Model Definition) has been run successfully first!")
else:
    # Define optimizer, loss, and metrics
    optimizer = keras.optimizers.Adam(learning_rate=0.001) # Adam with default LR is usually a good start
    loss_function = keras.losses.SparseCategoricalCrossentropy()
    metrics_to_track = ['accuracy']

    # Compile the model
    model.compile(optimizer=optimizer,
                  loss=loss_function,
                  metrics=metrics_to_track)

    print("Model compiled successfully.")
    print(f"  Optimizer: {type(optimizer).__name__} (lr={optimizer.learning_rate.numpy():.4f})") # Show LR
    print(f"  Loss Function: {type(loss_function).__name__}")
    print(f"  Metrics: {metrics_to_track}")

    print("\nReady for Model Training.")
    print("------------------------------------")

# Cell 8: Train the Model
# Based on cells [41-44] from Audio Training Notebook - Copy.txt

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import time # To time training

# Ensure model, training, and validation data exist from previous cells
if 'model' not in locals() \
   or 'X_train_cnn' not in locals() or 'y_train' not in locals() \
   or 'X_val_cnn' not in locals() or 'y_val' not in locals():
     print("ERROR: Ensure previous cells have run successfully first!")
     print("Needed: model, X_train_cnn, y_train, X_val_cnn, y_val")
else:
    # --- Training Configuration ---
    epochs = 50          # Max number of training iterations (might stop early)
    batch_size = 32      # Number of samples per gradient update
    # --------------------------

    # --- Define Callbacks ---
    # Save the best model based on validation loss
    model_checkpoint_callback = ModelCheckpoint(
        filepath='best_audio_classifier_model.keras', # File path to save the model
        save_weights_only=False,    # Save the full model (architecture + weights)
        monitor='val_loss',         # Metric to monitor
        mode='min',                 # We want to minimize validation loss
        save_best_only=True,        # Only save when validation loss improves
        verbose=1                   # Print messages when saving improved model
    )

    # Stop training early if validation loss doesn't improve
    early_stopping_callback = EarlyStopping(
        monitor='val_loss',     # Metric to monitor
        patience=10,            # Number of epochs with no improvement after which training stops
        verbose=1,              # Print messages when stopping
        restore_best_weights=True # Restore model weights from the epoch with the best val_loss
    )
    # -----------------


    print("\n--- Starting Model Training ---")
    print(f"Training on {X_train_cnn.shape[0]} samples, validating on {X_val_cnn.shape[0]} samples.")
    print(f"Max epochs: {epochs}, Batch size: {batch_size}.")
    print(f"Using Early Stopping (patience=10) monitoring 'val_loss'.")
    print(f"Saving the best model to 'best_audio_classifier_model.keras'.\n")

    # --- Train the model ---
    start_train_time = time.time()

    history = model.fit(
        X_train_cnn, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_val_cnn, y_val),
        callbacks=[model_checkpoint_callback, early_stopping_callback], # Pass the callbacks
        verbose=1 # Show progress bar per epoch
    )

    end_train_time = time.time()
    print(f"\n--- Model Training Finished ---")
    print(f"Total training time: {end_train_time - start_train_time:.2f} seconds")

    # You can inspect the history object later for plotting, etc.
    # print(history.history.keys())
    print("\nReady for Evaluation.")
    print("------------------------------------")

# Cell 9: Evaluate Model on Test Set
# Based on cell [46] from Audio Training Notebook - Copy.txt

import numpy as np # Keep import just in case

# Ensure model, X_test_cnn, and y_test exist from previous cells
if 'model' not in locals() \
   or 'X_test_cnn' not in locals() or 'y_test' not in locals():
     print("ERROR: Ensure previous cells have run successfully first!")
     print("Needed: model, X_test_cnn, y_test")
else:
    print("\n--- Evaluating Model on Test Set ---")

    # Evaluate the final model (with best weights restored) on the test data
    test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test, verbose=0) # verbose=0 for cleaner output

    print(f"\nTest Set Loss:     {test_loss:.4f}")
    print(f"Test Set Accuracy: {test_accuracy * 100:.2f}%") # Print accuracy as percentage

    print("\nReady for Visualization.")
    print("------------------------------------")

# Cell 10: Visualize Training History
# Based on cells [47-50] from Audio Training Notebook - Copy.txt

import matplotlib.pyplot as plt
import numpy as np # Keep import

# Ensure the 'history' object exists from the model.fit() call
if 'history' not in locals():
     print("ERROR: Ensure Cell 9 (Model Training) has been run successfully first!")
elif not hasattr(history, 'history'):
     print("ERROR: 'history' object does not contain training history data.")
else:
    print("\n--- Visualizing Training History ---")

    # Extract history data if available
    acc = history.history.get('accuracy', [])
    val_acc = history.history.get('val_accuracy', [])
    loss = history.history.get('loss', [])
    val_loss = history.history.get('val_loss', [])

    if not acc or not val_acc or not loss or not val_loss:
         print("ERROR: History object missing expected keys ('accuracy', 'val_accuracy', 'loss', 'val_loss').")
    else:
        epochs_range = range(len(acc)) # Use the actual number of epochs run

        plt.figure(figsize=(14, 5)) # Slightly wider figure

        # Plot Training & Validation Accuracy
        plt.subplot(1, 2, 1)
        plt.plot(epochs_range, acc, label='Training Accuracy', marker='.')
        plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='.')
        # Find the epoch with the best validation loss (weights restored from here)
        best_epoch = np.argmin(val_loss)
        plt.axvline(x=best_epoch, color='r', linestyle='--',
                    label=f'Best Epoch ({best_epoch+1} - val_loss={val_loss[best_epoch]:.4f})')
        plt.legend(loc='lower right')
        plt.title('Training and Validation Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.grid(True, linestyle='--')


        # Plot Training & Validation Loss
        plt.subplot(1, 2, 2)
        plt.plot(epochs_range, loss, label='Training Loss', marker='.')
        plt.plot(epochs_range, val_loss, label='Validation Loss', marker='.')
        plt.axvline(x=best_epoch, color='r', linestyle='--',
                    label=f'Best Epoch ({best_epoch+1} - val_loss={val_loss[best_epoch]:.4f})')
        plt.legend(loc='upper right')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, linestyle='--')


        plt.tight_layout() # Adjust layout
        plt.show()

        print(f"\nBest model weights were restored from epoch {best_epoch + 1} (where validation loss was lowest).")
        print("------------------------------------")# Cell 10: Visualize Training History
# Based on cells [47-50] from Audio Training Notebook - Copy.txt

import matplotlib.pyplot as plt
import numpy as np # Keep import

# Ensure the 'history' object exists from the model.fit() call
if 'history' not in locals():
     print("ERROR: Ensure Cell 9 (Model Training) has been run successfully first!")
elif not hasattr(history, 'history'):
     print("ERROR: 'history' object does not contain training history data.")
else:
    print("\n--- Visualizing Training History ---")

    # Extract history data if available
    acc = history.history.get('accuracy', [])
    val_acc = history.history.get('val_accuracy', [])
    loss = history.history.get('loss', [])
    val_loss = history.history.get('val_loss', [])

    if not acc or not val_acc or not loss or not val_loss:
         print("ERROR: History object missing expected keys ('accuracy', 'val_accuracy', 'loss', 'val_loss').")
    else:
        epochs_range = range(len(acc)) # Use the actual number of epochs run

        plt.figure(figsize=(14, 5)) # Slightly wider figure

        # Plot Training & Validation Accuracy
        plt.subplot(1, 2, 1)
        plt.plot(epochs_range, acc, label='Training Accuracy', marker='.')
        plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='.')
        # Find the epoch with the best validation loss (weights restored from here)
        best_epoch = np.argmin(val_loss)
        plt.axvline(x=best_epoch, color='r', linestyle='--',
                    label=f'Best Epoch ({best_epoch+1} - val_loss={val_loss[best_epoch]:.4f})')
        plt.legend(loc='lower right')
        plt.title('Training and Validation Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.grid(True, linestyle='--')


        # Plot Training & Validation Loss
        plt.subplot(1, 2, 2)
        plt.plot(epochs_range, loss, label='Training Loss', marker='.')
        plt.plot(epochs_range, val_loss, label='Validation Loss', marker='.')
        plt.axvline(x=best_epoch, color='r', linestyle='--',
                    label=f'Best Epoch ({best_epoch+1} - val_loss={val_loss[best_epoch]:.4f})')
        plt.legend(loc='upper right')
        plt.title('Training and Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, linestyle='--')


        plt.tight_layout() # Adjust layout
        plt.show()

        print(f"\nBest model weights were restored from epoch {best_epoch + 1} (where validation loss was lowest).")
        print("------------------------------------")

# Cell 11: Generate Confusion Matrix
# Based on cells [51-53] from Audio Training Notebook - Copy.txt

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Ensure model, test data, and label encoder exist
if 'model' not in locals() \
   or 'X_test_cnn' not in locals() or 'y_test' not in locals() \
   or 'label_encoder' not in locals():
     print("ERROR: Ensure previous cells have run successfully first!")
     print("Needed: model, X_test_cnn, y_test, label_encoder")
else:
    print("\n--- Generating Confusion Matrix for Test Set Performance ---")

    # Get model predictions on the test set
    y_pred_probs = model.predict(X_test_cnn)

    # Convert probabilities to predicted class index (0, 1, or 2)
    y_pred = np.argmax(y_pred_probs, axis=1)

    # Compute the confusion matrix
    cm = confusion_matrix(y_test, y_pred) # Compare true labels (y_test) with predicted labels (y_pred)

    # Get class names from the label encoder (should be 3 classes now)
    try:
        class_names = label_encoder.classes_
        if len(class_names) != 3:
             print(f"Warning: Expected 3 classes, but LabelEncoder has {len(class_names)}: {class_names}")
             # Fallback names if needed
             # class_names = ['Class 0', 'Class 1', 'Class 2']
        print(f"\nClass Names for Matrix: {class_names}")

        # Plot the confusion matrix
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

        fig, ax = plt.subplots(figsize=(6, 5)) # Adjusted size for 3x3
        disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='horizontal') # Horizontal labels are fine for 3 classes
        plt.title('Confusion Matrix - Test Set')
        # plt.tight_layout() # Often not needed with subplots(figsize=...)
        plt.show()

        print("\nConfusion Matrix explanation:")
        print("- Rows represent the True Labels.")
        print("- Columns represent the Predicted Labels.")
        print("- Values on the main diagonal (top-left to bottom-right) are correctly classified samples.")
        print("- Values off the main diagonal are misclassified samples.")

    except Exception as e:
        print(f"Error generating confusion matrix: {e}")

    print("\nReady to Save Model and Encoder.")
    print("------------------------------------")

# Cell 12: Save Final Model and Label Encoder
# Based on cells [56-57] from Audio Training Notebook - Copy.txt

import joblib
import tensorflow as tf # Ensure tensorflow is imported if needed for model saving context

# Ensure model and label_encoder exist and are the updated versions
if 'model' not in locals() or 'label_encoder' not in locals():
     print("ERROR: Ensure training finished and label_encoder is defined!")
else:
    # --- Save the retrained Keras Model ---
    # Note: The best version was already saved by ModelCheckpoint during training.
    # This saves the *final* state (which should have best weights restored).
    # Using a new name to distinguish the 3-class model.
    final_model_filename = 'audio_classifier_model_3class.keras'
    try:
        model.save(final_model_filename)
        print(f"\nFinal Keras model saved to: {final_model_filename}")
    except Exception as e:
        print(f"\nError saving final Keras model: {e}")


    # --- Save the 3-class LabelEncoder ---
    # This is essential for decoding predictions later
    encoder_filename = 'label_encoder_3class.joblib'
    print(f"\nSaving the 3-class LabelEncoder object...")
    try:
        joblib.dump(label_encoder, encoder_filename)
        print(f"  LabelEncoder successfully saved to: {encoder_filename}")
        # Verify the classes saved
        print(f"  Encoder classes saved: {list(label_encoder.classes_)}")
    except Exception as e:
        print(f"  Error saving LabelEncoder: {e}")

    print("\n------------------------------------")
    print("Retraining complete! Files saved.")
    print("------------------------------------")

# Cell 13: Function to Test Audio Files with Preprocessing and Energy Threshold

import librosa
import numpy as np
import os
import tensorflow as tf
import joblib
import math
import warnings
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Ensure warnings are managed if desired
# warnings.filterwarnings('ignore', category=UserWarning, module='librosa')

# --- Ensure necessary variables from previous cells are loaded ---
# model, label_encoder, target_sr, target_length_samples
# Also define the energy threshold to use for testing
ENERGY_THRESHOLD_CAR_HORN_TEST = 0.15 # Use the value you tuned, e.g., 0.15 or 0.20

# --- Re-define Helper functions used during preprocessing ---
# (Copied from Cell 1 for self-containment, ensure they match Cell 1 if updated)
def normalize_peak(y):
    peak = np.max(np.abs(y))
    if peak > 1e-5: return y / peak
    else: return y

def get_max_energy_chunk(y, sr, chunk_len_samples, target_len_samples):
    if len(y) <= target_len_samples:
         pad_width = target_len_samples - len(y)
         return np.pad(y.astype(np.float32), (0, pad_width), mode='constant', constant_values=0.0)
    frame_length = 2048; hop_length = 512
    rms_energy = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
    max_energy_frame_index = np.argmax(rms_energy)
    center_sample = librosa.frames_to_samples(max_energy_frame_index, hop_length=hop_length)
    start_sample = max(0, center_sample - (target_len_samples // 2))
    end_sample = start_sample + target_len_samples
    if end_sample > len(y): end_sample = len(y); start_sample = max(0, end_sample - target_len_samples)
    chunk = y[start_sample:end_sample]
    current_len = len(chunk)
    if current_len < target_len_samples: pad_width = target_len_samples - current_len; return np.pad(chunk.astype(np.float32), (0, pad_width), mode='constant', constant_values=0.0)
    elif current_len > target_len_samples: return chunk[:target_len_samples].astype(np.float32)
    else: return chunk.astype(np.float32)

# --- Main Evaluation Function ---
def evaluate_audio_files(file_paths, true_labels):
    """
    Loads, preprocesses, classifies audio files, applies energy threshold,
    and calculates accuracy against true labels.

    Args:
        file_paths (list): A list of strings containing paths to audio files.
        true_labels (list): A list of strings containing the corresponding true labels
                           (must be 'car_horn', 'emergency_vehicle', or 'traffic').
    """
    print(f"\n--- Evaluating {len(file_paths)} Audio Files ---")
    if 'model' not in globals() or 'label_encoder' not in globals():
        print("ERROR: Ensure 'model' and 'label_encoder' are loaded from previous cells.")
        return

    if len(file_paths) != len(true_labels):
        print("ERROR: Number of file paths must match number of true labels.")
        return

    predictions = []
    correct_predictions = 0
    model_input_shape = (1, 130, 20) # Should match model input

    # Get index mapping from the loaded 3-class encoder
    try:
        loaded_class_map = {name: i for i, name in enumerate(label_encoder.classes_)}
        idx_car_horn = loaded_class_map.get('car_horn', -1)
        idx_emergency = loaded_class_map.get('emergency_vehicle', -1)
        idx_traffic = loaded_class_map.get('traffic', -1)
        class_names = list(label_encoder.classes_) # For reports later
        print(f"Using LabelEncoder mapping: {loaded_class_map}")
        print(f"Using Energy Threshold for Car Horn (Index {idx_car_horn}): {ENERGY_THRESHOLD_CAR_HORN_TEST}")
        if -1 in [idx_car_horn, idx_emergency, idx_traffic]: raise ValueError("Encoder missing expected classes")
    except Exception as e:
        print(f"ERROR: Problem with loaded LabelEncoder: {e}")
        return

    for i, file_path in enumerate(file_paths):
        true_label = true_labels[i]
        print(f"\nProcessing: {os.path.basename(file_path)} (True Label: {true_label})")

        if not os.path.exists(file_path):
            print("  ERROR: File not found.")
            predictions.append("ERROR_FILE_NOT_FOUND") # Placeholder for error
            continue

        try:
            # 1. Load Full Audio
            y, sr = librosa.load(file_path, sr=target_sr, mono=True)

            # 2. Select/Fix Chunk
            # NOTE: This assumes we only want ONE chunk per file for evaluation,
            # similar to how 3s recordings are handled by the server.
            # If files are longer and *could* contain multiple events, this logic might need changing.
            if len(y) > target_length_samples and true_label == 'car_horn': # Smart chunk only for known long horns
                 final_chunk = get_max_energy_chunk(y, sr, target_length_samples, target_length_samples)
                 print("    Extracted max energy 3s chunk (as true label is car_horn)")
            else: # Pad/truncate others or short horns
                 current_len = len(y)
                 if current_len > target_length_samples: fixed_chunk = y[:target_length_samples]
                 elif current_len < target_length_samples: pad_width = target_length_samples - current_len; fixed_chunk = np.pad(y.astype(np.float32), (0, pad_width), mode='constant', constant_values=0.0)
                 else: fixed_chunk = y
                 final_chunk = fixed_chunk.astype(np.float32)

            # 3. Normalize
            y_normalized = normalize_peak(final_chunk)

            # 4. Calculate MFCCs
            mfcc = librosa.feature.mfcc(y=y_normalized, sr=target_sr, n_mfcc=20, n_fft=2048, hop_length=512)

            # 5. Reshape
            mfcc_reshaped = np.expand_dims(mfcc.T, axis=0)
            if mfcc_reshaped.shape != model_input_shape:
                 print(f"   ERROR: Unexpected MFCC shape {mfcc_reshaped.shape}. Skipping.")
                 predictions.append("ERROR_SHAPE")
                 continue

            # 6. Predict
            pred_probs = model.predict(mfcc_reshaped, verbose=0)
            pred_index = np.argmax(pred_probs, axis=1)[0]
            print(f"    Raw Probs: {pred_probs[0]}")
            print(f"    Predicted Index: {pred_index}")

            # 7. Calculate RMS
            rms = np.sqrt(np.mean(y_normalized**2)) # Use normalized chunk for RMS consistent check
            print(f"    RMS Energy (normalized chunk): {rms:.6f}")

            # 8. Map to Final Label with Energy Check
            final_prediction_label = None
            if pred_index == idx_emergency:
                final_prediction_label = 'emergency_vehicle'
            elif pred_index == idx_traffic:
                final_prediction_label = 'traffic'
            elif pred_index == idx_car_horn:
                if rms > ENERGY_THRESHOLD_CAR_HORN_TEST:
                    final_prediction_label = 'car_horn'
                    print(f"    CONFIRMED as car_horn (RMS > {ENERGY_THRESHOLD_CAR_HORN_TEST})")
                else:
                    print(f"    Predicted car_horn, but IGNORED (RMS <= {ENERGY_THRESHOLD_CAR_HORN_TEST})")
                    final_prediction_label = "background/ignored" # Assign temp label
            else:
                 print(f"    WARNING: Unknown prediction index {pred_index}")
                 final_prediction_label = "unknown"

            predictions.append(final_prediction_label)
            print(f"    Final Prediction: {final_prediction_label}")

            # Compare with true label
            if final_prediction_label == true_label:
                correct_predictions += 1
                print("    Result: CORRECT")
            else:
                # Handle ignored horns: if true label was car_horn, it's incorrect
                # If true label was something else (e.g. traffic) and it predicted horn but got ignored, that's correct.
                is_correct_ignore = (final_prediction_label == "background/ignored" and true_label != 'car_horn')
                if is_correct_ignore:
                     correct_predictions +=1
                     print("    Result: CORRECT (Ignored non-horn correctly)")
                else:
                     print(f"    Result: INCORRECT (Predicted: {final_prediction_label}, True: {true_label})")


        except Exception as e:
            print(f"  ERROR processing file {file_path}: {e}")
            predictions.append("ERROR_PROCESSING") # Placeholder for error


    # --- Calculate and Print Final Accuracy ---
    total_processed = len(predictions)
    # Exclude errors from accuracy calculation if needed, or count them as incorrect
    valid_predictions = [p for p in predictions if isinstance(p, str) and not p.startswith("ERROR_")]
    num_valid = len(valid_predictions)

    print(f"\n--- Evaluation Summary ---")
    if num_valid > 0:
         # Adjust true labels for ignored horns for accuracy calculation
         adjusted_true_labels = []
         for i, true_label in enumerate(true_labels):
              if i < len(predictions): # Check bounds
                   # If prediction was ignored, and true wasn't car_horn, treat true as 'ignored' for accuracy calc
                   if predictions[i] == 'background/ignored' and true_label != 'car_horn':
                        adjusted_true_labels.append('background/ignored')
                   else:
                        adjusted_true_labels.append(true_label)
              else: # Should not happen if lists match
                   adjusted_true_labels.append(true_label)


         # Calculate accuracy only on valid predictions where true label wasn't ignored background noise
         # Filter out errors before calculating accuracy
         filtered_preds = []
         filtered_true = []
         for i, p in enumerate(predictions):
              if isinstance(p, str) and not p.startswith("ERROR_"):
                  filtered_preds.append(p)
                  # Use adjusted true labels only up to the length of filtered_preds
                  if i < len(adjusted_true_labels):
                       filtered_true.append(adjusted_true_labels[i])

         if len(filtered_preds) == len(filtered_true) and len(filtered_preds) > 0:
               accuracy = accuracy_score(filtered_true, filtered_preds)
               print(f"Overall Accuracy: {accuracy * 100:.2f}% ({sum(1 for t, p in zip(filtered_true, filtered_preds) if t == p)} / {len(filtered_preds)})")

               # Add Classification Report and Confusion Matrix
               # Include 'background/ignored' if it occurred, otherwise use original 3 classes
               report_labels = sorted(list(set(filtered_true) | set(filtered_preds)))
               # Make sure only valid string labels are included
               report_labels = [l for l in report_labels if isinstance(l, str) and not l.startswith("ERROR_")]

               if report_labels: # Only show report if valid labels exist
                    print("\nClassification Report:")
                    # Handle potential zero_division issue if a class has no predictions/true samples in the test list
                    print(classification_report(filtered_true, filtered_preds, labels=report_labels, zero_division=0))

                    print("\nConfusion Matrix:")
                    try:
                        cm = confusion_matrix(filtered_true, filtered_preds, labels=report_labels)
                        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=report_labels)
                        fig, ax = plt.subplots(figsize=(7, 6))
                        disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation='horizontal')
                        plt.show()
                    except Exception as plot_err:
                        print(f"Could not plot confusion matrix: {plot_err}")
               else:
                    print("No valid labels found for classification report/matrix.")

         else:
              print("Could not calculate accuracy due to length mismatch or no valid predictions.")

    else:
        print("No valid predictions were made.")

    print("------------------------------------")

# Example: Create your test lists
test_files = [
    '/content/sounds/car_horn/sound_100.wav', # Assume this is a real horn
    '/content/sounds/emergency_vehicle/sound_1.wav', # Assume emergency
    '/content/sounds/traffic/sound_50.wav', # Assume traffic
    '/content/sounds/traffic/sound_51.wav' # Assume maybe a quiet traffic sound
]
true_labels_for_test = [
    'car_horn',
    'emergency_vehicle',
    'traffic',
    'traffic'
]

# Make sure the files actually exist!
existing_test_files = [f for f in test_files if os.path.exists(f)]
existing_true_labels = [true_labels_for_test[i] for i, f in enumerate(test_files) if os.path.exists(f)]

if existing_test_files:
     evaluate_audio_files(existing_test_files, existing_true_labels)
else:
     print("ERROR: None of the specified test files were found. Please check paths.")

# Cell: Convert Keras Model to TFLite (Targeting Built-in Ops)

import tensorflow as tf
import os
import numpy as np

# --- Configuration ---
keras_model_path = 'audio_classifier_model_3class.keras' # Your trained 3-class model
tflite_model_path = 'audio_classifier_model_3class_builtin_ops.tflite' # Use a new name
# --- --- --- --- --- ---

# --- Verify TF Version ---
print(f"Using TensorFlow version for conversion: {tf.__version__}")
# -----------------------

print(f"\n--- Converting Keras model to TFLite (BUILTINS Opset) ---")

if not os.path.exists(keras_model_path):
    print(f"ERROR: Keras model file not found at '{keras_model_path}'.")
else:
    try:
        # 1. Load Keras model
        print(f"Loading Keras model from: {keras_model_path}")
        loaded_keras_model = tf.keras.models.load_model(keras_model_path)
        print("Keras model loaded.")

        # 2. Create Converter
        converter = tf.lite.TFLiteConverter.from_keras_model(loaded_keras_model)

        # 3. *** Specify Target Ops ***
        # This restricts the converter to only use standard TFLite operations
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
        print("Set target_spec.supported_ops to TFLITE_BUILTINS for compatibility.")
        # ---------------------------

        # 4. Optimizations (Optional - try WITHOUT first, then maybe add DEFAULT back)
        # converter.optimizations = [tf.lite.Optimize.DEFAULT]
        # print("Optimizations DISABLED for this conversion attempt.")

        # 5. Convert
        print("Converting model...")
        tflite_model = converter.convert()
        print("Model converted successfully.")

        # 6. Save
        with open(tflite_model_path, 'wb') as f:
            f.write(tflite_model)
        print(f"TensorFlow Lite model (builtin ops) saved to: {tflite_model_path}")
        tflite_file_size = os.path.getsize(tflite_model_path) / 1024
        print(f"TFLite model size: {tflite_file_size:.2f} KB")

    except Exception as e:
        print(f"\nERROR during TFLite conversion: {e}", exc_info=True)

print("\n------------------------------------")
print("TFLite Conversion step finished.")
print("------------------------------------")